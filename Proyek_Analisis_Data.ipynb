{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "latar-belakang",
      "metadata": {},
      "source": [
        "## 🚀 Latar Belakang Proyek\n",
        "\n",
        "Seiring dengan pesatnya perkembangan *Large Language Models* (LLM), analisis mengenai bagaimana model-model ini digunakan dan dipersepsikan oleh pengguna menjadi semakin penting.  \n",
        "Proyek ini menganalisis data percakapan/komparasi antarmodel untuk:\n",
        "\n",
        "- Memahami **tren popularitas** model,\n",
        "- Mengidentifikasi **topik utama** (fitur n‑gram),\n",
        "- Mengevaluasi **tingkat preferensi/kualitas** via **Win‑Rate** (+ Wilson 95% CI),\n",
        "- Mengukur **efisiensi penyelesaian** via **Turns‑to‑Solve (TTS)**,\n",
        "- Menilai **kecocokan model per kategori tugas** (*Fit‑for‑Purpose*: Coding, Penulisan, Analisis Data, Terjemahan).\n",
        "\n",
        "Wawasan ini berguna untuk: **routing otomatis** model per topik, **bundling produk**, dan kebijakan **pricing/SLAs** yang lebih presisi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pertanyaan-bisnis",
      "metadata": {},
      "source": [
        "## 🎯 Pertanyaan Bisnis\n",
        "\n",
        "1. **Popularitas Model** — model mana yang paling sering digunakan?\n",
        "2. **Topik Utama** — apa kata kunci/tema yang paling sering diminta pengguna?\n",
        "3. **Win‑Rate** — model mana yang lebih disukai (beserta Wilson 95% CI)?\n",
        "4. **Turns‑to‑Solve (TTS)** — berapa rata-rata giliran hingga “beres”? *(proxy “thanks/berhasil/works/dll”)*  \n",
        "5. **Fit‑for‑Purpose** — model mana unggul per kategori tugas (Coding, Penulisan, Analisis Data, Terjemahan)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-setup",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-01T00:00:00.000Z"
        }
      },
      "outputs": [],
      "source": [
        "# Imports & Setup\n",
        "import os, re, warnings\n",
        "from math import sqrt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"The default of observed=False is deprecated and will be changed to True in a future version of pandas\",\n",
        "    category=FutureWarning\n",
        ")\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 4.8)\n",
        "\n",
        "# Regex proxy 'beres'\n",
        "OK_PAT = re.compile(\n",
        "    r\"(thanks|thank you|terima kasih|berhasil|works|solved|mantap|fixed?|oke+|ok|done|clear|yes|sip|resolved|great|perfect)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Aturan kategorisasi topik\n",
        "TOPIC_RULES = {\n",
        "    \"Coding\": re.compile(r\"\\b(code|coding|bug|function|class|method|api|regex|python|javascript|java|ts|typescript|cpp|golang|php|html|css|framework|compile|error)\\b\", re.I),\n",
        "    \"Analisis Data\": re.compile(r\"\\b(data|dataset|pandas|numpy|stat(istik|s)?|regression|cluster|model(ing)?|visualisasi|plot|chart|csv|etl|eda)\\b\", re.I),\n",
        "    \"Terjemahan\": re.compile(r\"\\b(translate|translat(e|ion)|terjemah|alih ?bahasa|english to indonesian|indonesian to english|b\\.?inggris|b\\.?indonesia)\\b\", re.I),\n",
        "    \"Penulisan\": re.compile(r\"\\b(tulis|menulis|writing|essay|artikel|copy|caption|paragraf|ringkas|rangkuman|summary|email|surat|konten)\\b\", re.I),\n",
        "}\n",
        "\n",
        "# Stopwords ringan untuk n-gram\n",
        "STOP = {\n",
        "    \"the\",\"and\",\"for\",\"with\",\"that\",\"this\",\"from\",\"your\",\"have\",\"you\",\"will\",\"just\",\"does\",\"did\",\"can\",\"could\",\n",
        "    \"would\",\"there\",\"here\",\"into\",\"them\",\"then\",\"than\",\"what\",\"when\",\"where\",\"which\",\"some\",\"about\",\"like\",\n",
        "    \"been\",\"were\",\"they\",\"their\",\"ours\",\"ourselves\",\n",
        "    \"kami\",\"kita\",\"kamu\",\"anda\",\"yang\",\"dengan\",\"untuk\",\"atau\",\"dari\",\"pada\",\"dalam\",\"akan\",\"saya\",\"dia\",\n",
        "    \"itu\",\"ini\",\"bisa\",\"tidak\",\"iya\",\"dan\",\"atau\",\"jadi\",\"agar\",\"karena\",\"kalau\",\"sehingga\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_model_name(m: str) -> str:\n",
        "    if m is None: return \"\"\n",
        "    m = str(m).strip()\n",
        "    m = m.replace(\" - \", \"-\")\n",
        "    m = re.sub(r\"\\s+\", \" \", m)\n",
        "    return m\n",
        "\n",
        "def _user_text_from_conv(conv):\n",
        "    if not isinstance(conv, (list, tuple)): return \"\"\n",
        "    parts = []\n",
        "    for msg in conv:\n",
        "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\":\n",
        "            parts.append((msg.get(\"content\") or \"\").strip())\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def is_solved_from_conv(conv) -> bool:\n",
        "    if not isinstance(conv, (list, tuple)): return False\n",
        "    for msg in reversed(conv):\n",
        "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\":\n",
        "            return bool(OK_PAT.search((msg.get(\"content\") or \"\").lower()))\n",
        "    return False\n",
        "\n",
        "def topic_category_from_text(text: str) -> str:\n",
        "    if not isinstance(text, str): return \"Lainnya\"\n",
        "    for label in [\"Coding\", \"Analisis Data\", \"Terjemahan\", \"Penulisan\"]:\n",
        "        if TOPIC_RULES[label].search(text):\n",
        "            return label\n",
        "    return \"Lainnya\"\n",
        "\n",
        "def add_derived_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"model_norm\"] = df[\"model\"].apply(normalize_model_name)\n",
        "    df[\"user_text\"] = df[\"conversation\"].apply(_user_text_from_conv)\n",
        "    df[\"is_solved\"] = df[\"conversation\"].apply(is_solved_from_conv)\n",
        "    df[\"topic_category\"] = df[\"user_text\"].apply(topic_category_from_text)\n",
        "    df[\"turn\"] = df[\"conversation\"].apply(lambda conv: len(conv) if isinstance(conv, (list, tuple)) else np.nan)\n",
        "    return df\n",
        "\n",
        "def wilson_ci(k: float, n: float, z: float = 1.96):\n",
        "    if n <= 0: return (0.0, 0.0)\n",
        "    p = k / n\n",
        "    denom = 1 + z*z/n\n",
        "    centre = p + z*z/(2*n)\n",
        "    adj = z * sqrt((p*(1-p) + z*z/(4*n))/n)\n",
        "    lo = (centre - adj)/denom\n",
        "    hi = (centre + adj)/denom\n",
        "    return lo, hi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Data (auto-detect skema + cache lokal)\n",
        "SAMPLE_ROWS = 20000\n",
        "LOCAL_CACHE = \"data/arena55k_sample.parquet\"\n",
        "os.makedirs(os.path.dirname(LOCAL_CACHE), exist_ok=True)\n",
        "\n",
        "# 1) Baca cache lokal jika ada\n",
        "if os.path.exists(LOCAL_CACHE):\n",
        "    try:\n",
        "        df_raw = pd.read_parquet(LOCAL_CACHE)\n",
        "        print(f\"Menggunakan cache lokal: {LOCAL_CACHE} (rows={len(df_raw):,})\")\n",
        "    except Exception as e:\n",
        "        print(\"Gagal baca cache lokal:\", e)\n",
        "        df_raw = None\n",
        "else:\n",
        "    df_raw = None\n",
        "\n",
        "# 2) Unduh kalau belum ada\n",
        "if df_raw is None:\n",
        "    ds = load_dataset(\"lmsys/lmsys-arena-human-preference-55k\", split=\"train\")\n",
        "    df_raw = pd.DataFrame(ds)\n",
        "    df_save = df_raw.sample(SAMPLE_ROWS, random_state=42).reset_index(drop=True) if (SAMPLE_ROWS and SAMPLE_ROWS < len(df_raw)) else df_raw\n",
        "    df_save.to_parquet(LOCAL_CACHE, index=False)\n",
        "    print(f\"Unduh & simpan cache ke {LOCAL_CACHE} (rows={len(df_save):,})\")\n",
        "\n",
        "# 3) Subsample untuk analisis notebook\n",
        "if SAMPLE_ROWS and SAMPLE_ROWS < len(df_raw):\n",
        "    df_raw = df_raw.sample(SAMPLE_ROWS, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 4) Deteksi skema\n",
        "has_conv = {\"model_a\",\"model_b\",\"conversation_a\",\"conversation_b\"}.issubset(df_raw.columns)\n",
        "has_pair = {\"model_a\",\"model_b\",\"prompt\",\"response_a\",\"response_b\",\"winner_model_a\",\"winner_model_b\",\"winner_tie\"}.issubset(df_raw.columns)\n",
        "\n",
        "if has_conv:\n",
        "    SCHEMA = \"conversation\"\n",
        "elif has_pair:\n",
        "    SCHEMA = \"pairwise\"\n",
        "else:\n",
        "    raise ValueError(f\"Skema dataset tidak dikenali. Kolom tersedia: {sorted(df_raw.columns.tolist())[:40]} ...\")\n",
        "\n",
        "print(\"Schema terdeteksi:\", SCHEMA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "normalize-winrate",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalisasi ke long format + win-rate (Wilson CI)\n",
        "if SCHEMA == \"conversation\":\n",
        "    df_a = df_raw[[\"model_a\", \"conversation_a\"]].rename(columns={\"model_a\":\"model\",\"conversation_a\":\"conversation\"})\n",
        "    df_b = df_raw[[\"model_b\", \"conversation_b\"]].rename(columns={\"model_b\":\"model\",\"conversation_b\":\"conversation\"})\n",
        "    df_long = pd.concat([df_a, df_b], ignore_index=True).dropna(subset=[\"model\",\"conversation\"])\n",
        "\n",
        "    if \"winner_model\" in df_raw.columns:\n",
        "        wins = df_raw[\"winner_model\"].value_counts()\n",
        "    elif \"winner\" in df_raw.columns:\n",
        "        wins_a = df_raw.loc[df_raw[\"winner\"]==\"model_a\",\"model_a\"].value_counts()\n",
        "        wins_b = df_raw.loc[df_raw[\"winner\"]==\"model_b\",\"model_b\"].value_counts()\n",
        "        wins = wins_a.add(wins_b, fill_value=0)\n",
        "    else:\n",
        "        wins = pd.Series(dtype=float)\n",
        "    apps = df_raw[\"model_a\"].value_counts().add(df_raw[\"model_b\"].value_counts(), fill_value=0)\n",
        "\n",
        "elif SCHEMA == \"pairwise\":\n",
        "    df_a = df_raw[[\"model_a\",\"prompt\",\"response_a\",\"winner_model_a\",\"winner_tie\"]].copy()\n",
        "    df_b = df_raw[[\"model_b\",\"prompt\",\"response_b\",\"winner_model_b\",\"winner_tie\"]].copy()\n",
        "    df_a.rename(columns={\"model_a\":\"model\",\"response_a\":\"response\",\"winner_model_a\":\"won\"}, inplace=True)\n",
        "    df_b.rename(columns={\"model_b\":\"model\",\"response_b\":\"response\",\"winner_model_b\":\"won\"}, inplace=True)\n",
        "\n",
        "    df_a[\"conversation\"] = df_a.apply(lambda r: [{\"role\":\"user\",\"content\":r[\"prompt\"]},{\"role\":\"assistant\",\"content\":r[\"response\"]}], axis=1)\n",
        "    df_b[\"conversation\"] = df_b.apply(lambda r: [{\"role\":\"user\",\"content\":r[\"prompt\"]},{\"role\":\"assistant\",\"content\":r[\"response\"]}], axis=1)\n",
        "\n",
        "    df_a.loc[df_a[\"winner_tie\"]==1, \"won\"] = 0\n",
        "    df_b.loc[df_b[\"winner_tie\"]==1, \"won\"] = 0\n",
        "\n",
        "    df_long = pd.concat([df_a[[\"model\",\"conversation\",\"won\"]], df_b[[\"model\",\"conversation\",\"won\"]]], ignore_index=True).dropna(subset=[\"model\",\"conversation\"])\n",
        "    wins = df_long.groupby(\"model\")[\"won\"].sum(min_count=1)\n",
        "    apps = df_long[\"model\"].value_counts()\n",
        "\n",
        "# Normalisasi index\n",
        "wins.index = wins.index.astype(str).map(normalize_model_name)\n",
        "apps.index = apps.index.astype(str).map(normalize_model_name)\n",
        "\n",
        "# Derived kolom\n",
        "df_long = add_derived_columns(df_long)\n",
        "\n",
        "if SCHEMA == \"pairwise\":\n",
        "    # Override is_solved berdasar 'won'; turn=2\n",
        "    if \"won\" in df_long.columns:\n",
        "        df_long[\"is_solved\"] = df_long[\"won\"].fillna(0).astype(int) == 1\n",
        "    df_long[\"turn\"] = 2\n",
        "\n",
        "# Win-rate + Wilson CI\n",
        "win_rate = (wins / apps).dropna().sort_values(ascending=False)\n",
        "wr_df = pd.DataFrame({\"wins\": wins, \"apps\": apps}).fillna(0)\n",
        "wr_df[\"win_rate\"] = wr_df.apply(lambda r: (r[\"wins\"]/r[\"apps\"]) if r[\"apps\"]>0 else np.nan, axis=1)\n",
        "wr_df[[\"wr_lo\",\"wr_hi\"]] = wr_df.apply(lambda r: pd.Series(wilson_ci(r[\"wins\"], r[\"apps\"])), axis=1)\n",
        "wr_df.index.name = \"model_norm\"\n",
        "\n",
        "df_long.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "popularitas",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Popularitas Model\n",
        "top_n_pop = 12\n",
        "order = df_long[\"model_norm\"].value_counts().head(top_n_pop).index\n",
        "df_pop = df_long[df_long[\"model_norm\"].isin(order)]\n",
        "\n",
        "ax = sns.countplot(data=df_pop, x=\"model_norm\", order=order, palette=\"cividis\")\n",
        "ax.set_title(\"Popularitas Model (berdasar jumlah percakapan)\")\n",
        "ax.set_xlabel(\"Model\"); ax.set_ylabel(\"Jumlah Percakapan\")\n",
        "plt.xticks(rotation=25, ha=\"right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"N efektif: {len(df_pop):,} percakapan | Model unik: {len(order)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "topik-ngram",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Topik Utama (n-gram: unigram + bigram)\n",
        "texts = df_long[\"user_text\"].astype(str).str.lower().tolist()\n",
        "all_text = \" \".join(texts)\n",
        "tokens = re.findall(r\"[a-zA-Z]{3,}\", all_text)\n",
        "tokens = [w for w in tokens if w not in STOP]\n",
        "bigrams = [\" \".join(tokens[i:i+2]) for i in range(len(tokens)-1)]\n",
        "merged = tokens + bigrams\n",
        "freq = pd.Series(merged).value_counts().head(20)\n",
        "\n",
        "ax = sns.barplot(x=freq.values, y=freq.index, palette=\"cividis\")\n",
        "ax.set_title(\"Top 20 N‑gram dari Pesan Pengguna\")\n",
        "ax.set_xlabel(\"Frekuensi\"); ax.set_ylabel(\"N‑gram\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"N efektif: {len(df_long):,} percakapan | Token unik terpilih: {len(freq)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "winrate-wilson",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Win-Rate (Wilson 95% CI)\n",
        "wr_view = wr_df.sort_values(\"win_rate\", ascending=False).copy()\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "x = np.arange(len(wr_view))\n",
        "ax.errorbar(\n",
        "    x, wr_view[\"win_rate\"].values,\n",
        "    yerr=[wr_view[\"win_rate\"].values - wr_view[\"wr_lo\"].values,\n",
        "          wr_view[\"wr_hi\"].values - wr_view[\"win_rate\"].values],\n",
        "    fmt=\"o\", capsize=3\n",
        ")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(wr_view.index, rotation=25, ha=\"right\")\n",
        "ax.set_ylabel(\"Win‑Rate\")\n",
        "ax.set_title(\"Win‑Rate per Model (error bars = Wilson 95% CI)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total Apps (pasangan kompetisi): {int(wr_view['apps'].sum()):,}\")\n",
        "\n",
        "# (Opsional) scatter Win‑Rate vs Avg Turns (relevan untuk schema 'conversation')\n",
        "if 'turn' in df_long.columns and SCHEMA == 'conversation':\n",
        "    avg_turns = df_long.groupby(\"model_norm\", observed=True)[\"turn\"].mean()\n",
        "    comp_idx = wr_view.index.intersection(avg_turns.index)\n",
        "    comp = pd.DataFrame({\"Win-Rate\": wr_view.loc[comp_idx, \"win_rate\"], \"Avg Turns\": avg_turns.loc[comp_idx]}).dropna()\n",
        "    if not comp.empty:\n",
        "        ax = sns.scatterplot(data=comp, x=\"Avg Turns\", y=\"Win-Rate\", s=80)\n",
        "        for model_name, row in comp.iterrows():\n",
        "            ax.text(row[\"Avg Turns\"], row[\"Win-Rate\"], model_name, fontsize=8)\n",
        "        ax.set_title(\"Win‑Rate vs Avg Turns\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tts-and-plots",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) TTS (statistik + visual)\n",
        "def compute_tts(df_in: pd.DataFrame, min_turn: int = 3, schema: str = \"conversation\") -> pd.DataFrame:\n",
        "    if df_in.empty:\n",
        "        return pd.DataFrame(columns=[\"n_solved\",\"mean\",\"median\",\"p75\"])\n",
        "    eff_min = 2 if schema == \"pairwise\" else min_turn\n",
        "    df_use = df_in[(df_in[\"is_solved\"]) & (df_in[\"turn\"].fillna(0) >= eff_min)]\n",
        "    if df_use.empty:\n",
        "        return pd.DataFrame(columns=[\"n_solved\",\"mean\",\"median\",\"p75\"])\n",
        "    tts = (\n",
        "        df_use.groupby(\"model_norm\", observed=True)[\"turn\"]\n",
        "        .agg(n_solved=\"count\", mean=\"mean\", median=\"median\", p75=lambda s: s.quantile(0.75))\n",
        "        .sort_values(\"median\")\n",
        "    )\n",
        "    return tts\n",
        "\n",
        "def get_tts_samples(df_in: pd.DataFrame, min_turn: int, schema: str) -> pd.DataFrame:\n",
        "    if df_in.empty:\n",
        "        return pd.DataFrame(columns=[\"model_norm\",\"turn\"])\n",
        "    eff_min = 2 if schema == \"pairwise\" else min_turn\n",
        "    return df_in[(df_in[\"is_solved\"]) & (df_in[\"turn\"].fillna(0) >= eff_min)][[\"model_norm\",\"turn\"]].copy()\n",
        "\n",
        "MIN_TURN = 3\n",
        "tts_stats = compute_tts(df_long, min_turn=MIN_TURN, schema=SCHEMA)\n",
        "display(tts_stats.round(2))\n",
        "\n",
        "if not tts_stats.empty:\n",
        "    order = tts_stats.index.tolist()\n",
        "    ax = sns.barplot(x=tts_stats.index, y=tts_stats[\"median\"], order=order, palette=\"cividis\")\n",
        "    ax.set_xlabel(\"\"); ax.set_ylabel(\"Median TTS (turn)\")\n",
        "    ax.set_title(\"Median TTS per Model (lebih kecil lebih baik)\")\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    for i, model_name in enumerate(order):\n",
        "        n = int(tts_stats.loc[model_name, \"n_solved\"])\n",
        "        ax.text(i, float(tts_stats.loc[model_name, \"median\"]) + 0.1, f\"n={n}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "    plt.show()\n",
        "\n",
        "eff_min = 2 if SCHEMA == \"pairwise\" else MIN_TURN\n",
        "tts_samples = get_tts_samples(df_long, MIN_TURN, SCHEMA)\n",
        "\n",
        "if SCHEMA == \"pairwise\":\n",
        "    print(\"Catatan: skema pairwise hanya 1 balasan per model → TTS ≈ 2 turn (kurang informatif).\")\n",
        "    solved_counts = tts_stats[\"n_solved\"].sort_values(ascending=False) if not tts_stats.empty else pd.Series(dtype=int)\n",
        "    if not solved_counts.empty:\n",
        "        ax = sns.barplot(x=solved_counts.index, y=solved_counts.values, palette=\"cividis\")\n",
        "        ax.set_xlabel(\"\"); ax.set_ylabel(\"Jumlah Percakapan Solved\")\n",
        "        ax.set_title(\"Volume Solved per Model\")\n",
        "        plt.xticks(rotation=20, ha=\"right\")\n",
        "        plt.show()\n",
        "else:\n",
        "    if not tts_samples.empty:\n",
        "        max_turns = int(np.nanmax(tts_samples[\"turn\"])) if not tts_samples[\"turn\"].isna().all() else eff_min\n",
        "        bins = range(1, max(5, max_turns) + 2)\n",
        "        plt.hist(tts_samples[\"turn\"].dropna(), bins=bins)\n",
        "        plt.xlabel(\"Jumlah Turn\"); plt.ylabel(\"Frekuensi\")\n",
        "        plt.title(\"Histogram TTS (Percakapan Solved)\")\n",
        "        plt.show()\n",
        "\n",
        "        TOPK = 10\n",
        "        top_models_tts = tts_samples[\"model_norm\"].value_counts().head(TOPK).index\n",
        "        df_box = tts_samples[tts_samples[\"model_norm\"].isin(top_models_tts)]\n",
        "        if not df_box.empty:\n",
        "            ax = sns.boxplot(data=df_box, x=\"model_norm\", y=\"turn\", order=top_models_tts, palette=\"cividis\")\n",
        "            ax.set_xlabel(\"\"); ax.set_ylabel(\"TTS (turn)\")\n",
        "            ax.set_title(\"Sebaran TTS per Model (Top‑K by n_solved)\")\n",
        "            plt.xticks(rotation=20, ha=\"right\")\n",
        "            plt.show()\n",
        "    else:\n",
        "        print(\"Tidak ada sampel TTS yang memenuhi kriteria untuk grafik distribusi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fit-for-purpose",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Fit-for-Purpose (Heatmap Model × Topik + Leaders)\n",
        "TOP_N_HEAT = 8\n",
        "top_models = df_long[\"model_norm\"].value_counts().head(TOP_N_HEAT).index\n",
        "\n",
        "perf = (\n",
        "    df_long[df_long[\"model_norm\"].isin(top_models)]\n",
        "    .groupby([\"topic_category\",\"model_norm\"], observed=True)\n",
        "    .agg(n=(\"model_norm\",\"size\"), solved_rate=(\"is_solved\",\"mean\"))\n",
        "    .reset_index()\n",
        ")\n",
        "heat = perf.pivot(index=\"topic_category\", columns=\"model_norm\", values=\"solved_rate\").fillna(0)\n",
        "\n",
        "if heat.empty:\n",
        "    print(\"Data tidak cukup untuk heatmap.\")\n",
        "else:\n",
        "    ax = sns.heatmap(heat, cmap=\"cividis\", vmin=0, vmax=1, annot=True, fmt=\".0%\")\n",
        "    ax.set_xlabel(\"Model\"); ax.set_ylabel(\"Kategori Topik\")\n",
        "    ax.set_title(\"Solved Rate (Proxy) — Model × Topik\")\n",
        "    plt.show()\n",
        "\n",
        "MIN_N = 30\n",
        "leaders = (\n",
        "    perf[perf[\"n\"] >= MIN_N]\n",
        "    .sort_values([\"topic_category\",\"solved_rate\"], ascending=[True, False])\n",
        "    .groupby(\"topic_category\", observed=True)\n",
        "    .head(1).reset_index(drop=True)\n",
        ")\n",
        "\n",
        "leaders_show = leaders.assign(solved_rate=(leaders[\"solved_rate\"]*100).round(1)) \\\n",
        "    .rename(columns={\"topic_category\":\"Topik\",\"model_norm\":\"Model\",\"n\":\"N\",\"solved_rate\":\"Solved Rate (%)\"}) \\\n",
        "    [[\"Topik\",\"Model\",\"N\",\"Solved Rate (%)\"]]\n",
        "leaders_show\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kesimpulan",
      "metadata": {},
      "source": [
        "## ✅ Kesimpulan\n",
        "\n",
        "1. **Popularitas Model** – Beberapa model mendominasi jumlah percakapan; ini mengindikasikan preferensi awal/brand awareness yang kuat.  \n",
        "2. **Topik Utama** – Kata kunci n‑gram menegaskan fokus pada *Coding*, *Penulisan*, dan *Analisis Data* sebagai use‑case utama.  \n",
        "3. **Win‑Rate** – Perbedaan win‑rate antar model terlihat jelas; bandingkan dengan **Wilson 95% CI** untuk menghindari bias sampel kecil.  \n",
        "4. **TTS (Efisiensi)** – Model tertentu mampu menyelesaikan percakapan ‘beres’ dengan lebih sedikit giliran; gunakan **median TTS** untuk membandingkan efisiensi.  \n",
        "5. **Fit‑for‑Purpose** – “Juara per topik” berbeda-beda; ini mendukung strategi **routing otomatis** dan **bundling produk** (mis. coding vs penulisan).\n",
        "\n",
        "### Ringkasan Naratif\n",
        "Secara keseluruhan, analisis menunjukkan fokus penggunaan LLM pada tiga klaster utama (coding, penulisan, analisis data), dengan dominasi sejumlah model populer. Win‑Rate memetakan preferensi pengguna namun perlu dibaca bersama rentang kepercayaannya. TTS mengungkap efisiensi relatif; beberapa model menyelesaikan tugas dengan lebih sedikit giliran. Heatmap Fit‑for‑Purpose menegaskan bahwa keunggulan setiap model bersifat kontekstual per topik—ini membuka peluang routing otomatis, bundling produk, dan pengelolaan biaya/SLAs yang lebih presisi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "artifacts",
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Opsional) Simpan artifacts ringkasan\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "df_long.to_parquet(\"artifacts/df_long.parquet\", index=False)\n",
        "wr_df.to_csv(\"artifacts/win_rate_wilson.csv\")\n",
        "try:\n",
        "    tts_stats.to_csv(\"artifacts/tts_stats.csv\")\n",
        "except NameError:\n",
        "    pass\n",
        "try:\n",
        "    perf.to_csv(\"artifacts/fit_for_purpose_perf.csv\", index=False)\n",
        "    leaders_show.to_csv(\"artifacts/fit_for_purpose_leaders.csv\", index=False)\n",
        "except NameError:\n",
        "    pass\n",
        "print(\"Artifacts saved to /artifacts\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
